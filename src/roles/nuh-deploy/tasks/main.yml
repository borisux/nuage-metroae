---
- name: Clean known_hosts of NUH's (ignoring errors)
  known_hosts:
    name: "{{ hostname }}"
    state: absent
  delegate_to: localhost
  no_log: True
  ignore_errors: True

- name: Wait for NUH ssh to be ready
  include_role:
    name: common
    tasks_from: wait-for-ssh
  vars:
    ssh_host: "{{ hostname }}"
    host_username: "root"

- block:

  - block:

    - name: Copy license file
      copy:
        src: "{{ nuh_license_file }}"
        dest: "/opt/proxy/nuh.license"

    - name: Verify License is valid
      shell: java -jar /opt/proxy/bin/lm/LicenseManager.jar features

    when: nuh_license_file is defined

  - name: Run the setup script for SA
    command: ./setup.sh
    environment:
      HNAME: "{{ hostname }}"
      HA: "n"
    args:
      chdir: /opt/proxy/bin
    when: nuh_sa_or_ha is match('sa')

  - name: Run the setup script for HA primary
    command: ./setup.sh
    environment:
      HNAME: "{{ hostname }}"
      HA: "y"
      MASTER: "y"
      PEERADDR: "{{ groups['nuhs'][1] }}"
      PEERPASSWORD: "{{ nuh_default_password }}"
    args:
      chdir: /opt/proxy/bin
    when:
      - nuh_sa_or_ha is match('ha')
      - inventory_hostname == groups['nuhs'][0]

  - name: Run the setup script for HA secondary
    command: ./setup.sh
    environment:
      HNAME: "{{ hostname }}"
      HA: "y"
      MASTER: "n"
      PEERADDR: "{{ groups['nuhs'][0] }}"
      PEERPASSWORD: "{{ nuh_default_password }}"
    args:
      chdir: /opt/proxy/bin
    when:
      - nuh_sa_or_ha is match('ha')
      - inventory_hostname == groups['nuhs'][1]

  - name: Set the timezone
    command: timedatectl set-timezone {{ nuh_timezone }}

  - name: Create and transfer certs
    include_role:
      name: common
      tasks_from: vsd-generate-transfer-certificates
    when: not skip_vsd_installed_check
    vars:
      certificate_password: "{{ nuh_default_password }}"
      certificate_username: "{{ inventory_hostname }}"
      commonName: "{{ inventory_hostname }}"
      certificate_type: server
      scp_user: "root"
      scp_location: /opt/proxy/data/certs
      additional_parameters:  -d {{ inventory_hostname }}

  - block:

    - name: Get vsd node(s) information
      import_role:
        name: common
        tasks_from: vsd-node-info.yml
      vars:
        vsd_hostname: "{{ vsd_fqdn }}"

    - block:

      - name: Create and transfer certs
        include_role:
          name: common
          tasks_from: vsd-generate-transfer-certificates
        when: not skip_vsd_installed_check
        vars:
          certificate_password: "{{ nuh_default_password }}"
          certificate_username: "nuhproxy"
          commonName: "nuhproxy"
          certificate_type: server
          scp_user: "root"
          scp_location: /opt/proxy/data/certs
          additional_parameters:  -d {{ item.external_fqdn }}
        with_items:
          - "{{ external_interfaces }}"

      - name: Get vsd node(s) information
        import_role:
          name: common
          tasks_from: vsd-node-info.yml
        vars:
          vsd_hostname: "{{ vsd_fqdn }}"
        run_once: true

      - name: Get custom username for vsd
        set_fact:
          custom_username: "{{ hostvars[groups['vsd_ha_node1'][0]].vsd_custom_username | default(vsd_custom_username) }}"
          custom_password: "{{ hostvars[groups['vsd_ha_node1'][0]].vsd_custom_password | default(vsd_custom_password) }}"
        when:
          - hostvars[groups['vsd_ha_node1'][0]].vsd_custom_username is defined

      - name: Create notification application user on vsd
        command: /opt/ejabberd/bin/ejabberdctl register {{ item.username }} {{ xmpp_domain }} {{ item.password }}
        delegate_to: vsd_hostname_list[0]
        run_once: true
        with_items:
          - notification_app1
          - notification_app2
        remote_user: "{{ custom_username | default(vsd_default_username) }}"
        become: "{{ 'no' if custom_username | default(vsd_default_username) == 'root' else 'yes' }}"
        vars:
          ansible_become_pass: "{{ custom_password | default(vsd_default_password) }}"

      - name: Copy the network config for NUH internal and external networks
        template: src=config.yml.j2 backup=no dest=/opt/proxy/data/config.yml owner=root group=root mode=0640

      - name: Update haproxy Service
        lineinfile:
          path: /opt/proxy/bin/ansible/roles/common/handlers/main.yml
          regex: "shell: systemctl reload-or-restart haproxy"
          line: "  shell: systemctl restart haproxy"

      - name: Run the network config script on NUH
        command: ansible-playbook configure.yml
        args:
          chdir: /opt/proxy/bin/ansible

      - name: Reset haproxy Service
        lineinfile:
          path: /opt/proxy/bin/ansible/roles/common/handlers/main.yml
          regex: "shell: systemctl restart haproxy"
          line: "  shell: systemctl reload-or-restart haproxy"

      - name: Check if the eth1 is configured on Internal interface
        shell: set -o pipefail && ip netns exec Internal ip addr list | grep eth1

      - name: Check if the eth2 is configured on External interfaces
        shell: set -o pipefail && ip netns exec {{ item.name }} ip addr list | grep eth2
        with_items: "{{ external_interfaces }}"
        when: external_interfaces is defined and external_interfaces | length > 0

      when: inventory_hostname == groups['nuhs'][0]

    when:
      - internal_ip is defined
      - not skip_vsd_installed_check

  - block:

      - name: Copy the Custom configuration file if provided by user
        copy:
          dest: "/opt/proxy/data/config.yml"
          src: "{{ custom_configuration_file_location }}"
          mode: 0640
          owner: "root"
          group: "root"
        when: custom_configuration_file_location is defined

      - block:

        - name: Add stats-out proxy entries to NUH configuration
          replace:
            path: /opt/proxy/data/config.yml
            regexp: "role: vsdconfig.*$"
            replace: "role: vsdconfig, enabled: true, firewallports: ['{{ stats_out_proxy_ui_port }}', '{{ stats_out_proxy_api_port }}', '{{ stats_out_proxy_jms_port }}', '{{ stats_out_proxy_xmpp_port }}'], settings: {uiport: '{{ stats_out_proxy_ui_port }}', apiport: '{{ stats_out_proxy_api_port }}', xmppport: '{{ stats_out_proxy_xmpp_port }}', jmsport: '{{ stats_out_proxy_jms_port }}', geo: false}}"
          when: stats_out_proxy | default('NONE') in [hostname, mgmt_ip]

        when: NUH_DOES_NOT_YET_SUPPORT_STATS_OUT_PROXY | default(false)

      - name: Run the configuration script on NUH
        command: ansible-playbook configure.yml
        args:
          chdir: /opt/proxy/bin/ansible

    when:
      - custom_configuration_file_location is defined or stats_out_proxy | default('NONE') in [hostname, mgmt_ip]
      - inventory_hostname == groups['nuhs'][0]
  remote_user: "{{ nuh_default_username }}"

- name: Setup health monitoring
  include_role:
    name: setup-health-monitoring
  vars:
    component_username: "{{ nuh_default_username }}"
  when: health_monitoring_agent | default("none") != "none"
