---
- name: Clean known_hosts of NUH's (ignoring errors)
  known_hosts:
    name: "{{ hostname }}"
    state: absent
  delegate_to: localhost
  no_log: True
  ignore_errors: True

- name: Wait for NUH ssh to be ready
  include_role:
    name: common
    tasks_from: wait-for-ssh
  vars:
    ssh_host: "{{ hostname }}"
    host_username: "root"

- name: Check if ha proxy is running (ignoring errors)
  command: systemctl status haproxy
  ignore_errors: true
  register: haproxy_output
  remote_user: "{{ nuh_default_username }}"

- name: Define skip nuh installation
  set_fact:
    skip_nuh_deploy: "{{ haproxy_output.stdout.find('active (running)') != -1 }}"

- name: Display if skipping NUH deploy
  debug:
    msg:
      - "*************************************************"
      - "Skipping NUH deploy because it is already running"
      - "*************************************************"
  when: skip_nuh_deploy

- block:

  - block:

    - name: Copy license file
      copy:
        src: "{{ nuh_license_file }}"
        dest: "/opt/proxy/nuh.license"

    - name: Verify License is valid
      shell: java -jar /opt/proxy/bin/lm/LicenseManager.jar features

    when: nuh_license_file is defined

  - name: Run the setup script for SA
    command: ./setup.sh
    environment:
      HNAME: "{{ hostname }}"
      HA: "n"
    args:
      chdir: /opt/proxy/bin
    when: nuh_sa_or_ha is match('sa')

  - name: Run the setup script for HA primary
    command: ./setup.sh
    environment:
      HNAME: "{{ hostname }}"
      HA: "y"
      MASTER: "y"
      PEERADDR: "{{ groups['nuhs'][1] }}"
      PEERPASSWORD: "{{ nuh_default_password }}"
    args:
      chdir: /opt/proxy/bin
    when:
      - nuh_sa_or_ha is match('ha')
      - inventory_hostname == groups['nuhs'][0]

  - name: Run the setup script for HA secondary
    command: ./setup.sh
    environment:
      HNAME: "{{ hostname }}"
      HA: "y"
      MASTER: "n"
      PEERADDR: "{{ groups['nuhs'][0] }}"
      PEERPASSWORD: "{{ nuh_default_password }}"
    args:
      chdir: /opt/proxy/bin
    when:
      - nuh_sa_or_ha is match('ha')
      - inventory_hostname == groups['nuhs'][1]

  - name: Set the timezone
    command: timedatectl set-timezone {{ nuh_timezone }}

  - name: Create and transfer certs
    include_role:
      name: common
      tasks_from: vsd-generate-transfer-certificates
    when: not skip_vsd_installed_check
    vars:
      certificate_password: "{{ nuh_default_password }}"
      certificate_username: "{{ inventory_hostname }}"
      commonName: "{{ inventory_hostname }}"
      certificate_type: server
      scp_user: "root"
      scp_location: /opt/proxy/data/certs
      additional_parameters:  -d {{ inventory_hostname }}

  - block:

    - name: Get vsd node(s) information
      import_role:
        name: common
        tasks_from: vsd-node-info.yml
      vars:
        vsd_hostname: "{{ vsd_fqdn }}"

    - block:

      - name: Create and transfer certs
        include_role:
          name: common
          tasks_from: vsd-generate-transfer-certificates
        when: not skip_vsd_installed_check
        vars:
          certificate_password: "{{ nuh_default_password }}"
          certificate_username: "proxy"
          commonName: "proxy"
          certificate_type: server
          scp_user: "root"
          scp_location: /opt/proxy/data/certs
          additional_parameters:  -d {{ item.external_fqdn }}
        with_items:
          - "{{ external_interfaces }}"

      - name: Copy proxy certificates over to secondary nuh
        synchronize:
          src: "{{ item }}"
          dest: "{{ item }}"
          mode: pull
          private_key: "/root/.ssh/proxypeer"
        delegate_to: "{{ groups['nuhs'][1] }}"
        run_once: true
        when: groups['nuhs'] | length > 1
        with_items:
          - "/opt/proxy/data/certs/proxy-CA.pem"
          - "/opt/proxy/data/certs/proxyCert.pem"
          - "/opt/proxy/data/certs/proxy-Key.pem"
          - "/opt/proxy/data/certs/proxy.pem"

      - block:
        - name: Create notification application user on vsd
          command: /opt/ejabberd/bin/ejabberdctl register {{ notification_app1.username }} {{ vsd_fqdn }} {{ notification_app1.password }}
          run_once: true
          no_log: "{{ lookup('env', 'METROAE_NO_LOG') or 'true' }}"
          when: notification_app1 is defined

        - name: Create notification application user on vsd
          command: /opt/ejabberd/bin/ejabberdctl register {{ notification_app2.username }} {{ vsd_fqdn }} {{ notification_app2.password }}
          run_once: true
          no_log: "{{ lookup('env', 'METROAE_NO_LOG') or 'true' }}"
          when: notification_app2 is defined

        delegate_to: "{{ vsd_hostname_list[0] }}"
        remote_user: "{{ custom_username | default(vsd_default_username) }}"
        become: "{{ 'no' if custom_username | default(vsd_default_username) == 'root' else 'yes' }}"
        vars:
          ansible_become_pass: "{{ custom_password | default(vsd_default_password) }}"

      - name: Copy the network config for NUH internal and external networks
        template: src=config.yml.j2 backup=no dest=/opt/proxy/data/config.yml owner=root group=root mode=0640
        no_log: "{{ lookup('env', 'METROAE_NO_LOG') or 'true' }}"

      - name: Update haproxy Service
        lineinfile:
          path: /opt/proxy/bin/ansible/roles/common/handlers/main.yml
          regex: "shell: systemctl reload-or-restart haproxy"
          line: "  shell: systemctl restart haproxy"

      - name: Run the network config script on NUH
        command: ansible-playbook configure.yml
        args:
          chdir: /opt/proxy/bin/ansible

      - name: Reset haproxy Service
        lineinfile:
          path: /opt/proxy/bin/ansible/roles/common/handlers/main.yml
          regex: "shell: systemctl restart haproxy"
          line: "  shell: systemctl reload-or-restart haproxy"

      - name: Check if the eth1 is configured on Internal interface
        shell: set -o pipefail && ip netns exec Internal ip addr list | grep eth1

      - name: Check if the eth2 is configured on External interfaces
        shell: set -o pipefail && ip netns exec {{ item.name }} ip addr list | grep eth2
        with_items: "{{ external_interfaces }}"
        when: external_interfaces is defined and external_interfaces | length > 0

      when: inventory_hostname == groups['nuhs'][0]

    when:
      - internal_ip is defined
      - not skip_vsd_installed_check

  - block:

      - name: Copy the Custom configuration file if provided by user
        copy:
          dest: "/opt/proxy/data/config.yml"
          src: "{{ custom_configuration_file_location }}"
          mode: 0640
          owner: "root"
          group: "root"
        when: custom_configuration_file_location is defined

      - block:

        - name: Add stats-out proxy entries to NUH configuration
          replace:
            path: /opt/proxy/data/config.yml
            regexp: "role: vsdconfig.*$"
            replace: "role: vsdconfig, enabled: true, firewallports: ['{{ stats_out_proxy_ui_port }}', '{{ stats_out_proxy_api_port }}', '{{ stats_out_proxy_jms_port }}', '{{ stats_out_proxy_xmpp_port }}', '{{ stats_out_proxy_cert_port }}'], settings: {uiport: '{{ stats_out_proxy_ui_port }}', apiport: '{{ stats_out_proxy_api_port }}', xmppport: '{{ stats_out_proxy_xmpp_port }}', jmsport: '{{ stats_out_proxy_jms_port }}', geo: false, certport: '{{ stats_out_proxy_cert_port }}'}}"

        - name: Add stats-out proxy entries to NUH configuration
          blockinfile:
            path: /opt/proxy/data/config.yml
            marker: ""
            block: |
              nsgstats:
                {% for vstat in groups['vstats'] | list + groups['data_vstats'] | list %}
                - {{ hostvars[vstat]['mgmt_ip'] }}
                {% endfor %}

        when: stats_out_proxy | default('NONE') == internal_ip | default("not_set")

      - name: Run the configuration script on NUH
        command: ansible-playbook configure.yml
        args:
          chdir: /opt/proxy/bin/ansible

    when:
      - custom_configuration_file_location is defined or stats_out_proxy | default('NONE') == internal_ip | default("not_set")
      - inventory_hostname == groups['nuhs'][0]

  when:
    - not skip_nuh_deploy
  remote_user: "{{ nuh_default_username }}"

- name: Setup health monitoring
  include_role:
    name: setup-health-monitoring
  vars:
    component_username: "{{ nuh_default_username }}"
  when: health_monitoring_agent | default("none") != "none"
